# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N6E6ECNTPDbgn50CZj26lnszhO72ZKLm
"""

!pip install -q transformers datasets accelerate

from transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

model = GPT2LMHeadModel.from_pretrained("gpt2")
model.resize_token_embeddings(len(tokenizer))

!pip install pandas

import csv

texts = []

with open("train.csv", encoding="utf-8", errors="ignore") as f:
    reader = csv.DictReader(f)
    for row in reader:
        if "text" in row and row["text"]:
            texts.append(row["text"].replace("\n", " "))

with open("validation.csv", encoding="utf-8", errors="ignore") as f:
    reader = csv.DictReader(f)
    for row in reader:
        if "text" in row and row["text"]:
            texts.append(row["text"].replace("\n", " "))

print("Total stories:", len(texts))

print("Total stories extracted:", len(texts))

import random

for i in random.sample(range(len(texts)), 3):
    print("\n--- STORY SAMPLE ---")
    print(texts[i][:500])

lengths = [len(t.split()) for t in texts]

print("Min words:", min(lengths))
print("Max words:", max(lengths))
print("Average words:", sum(lengths) // len(lengths))

texts = [t for t in texts if len(t.split()) > 20]
print("After cleaning:", len(texts))

with open("training_data.txt", "w", encoding="utf-8") as f:
    for story in texts:
        f.write(story.strip() + "\n")

!wc -l training_data.txt

!head -n 3 training_data.txt

!nvidia-smi

from datasets import load_dataset

dataset = load_dataset(
    "text",
    data_files={"train": "training_data.txt"}
)

print(dataset)
print(dataset["train"][0]["text"][:300])

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["text"]
)

from transformers import TrainingArguments

#training_args = TrainingArguments(
 #   output_dir="./gpt2-tinystories",
  #  overwrite_output_dir=True,
  #  num_train_epochs=1,
   # max_steps=5000,
   # per_device_train_batch_size=2,
   # gradient_accumulation_steps=4,
   # save_steps=5000,
   # save_total_limit=2,
   # logging_steps=1000,
   # learning_rate=5e-5,
   # fp16=True,
   # report_to="none"
#)
training_args = TrainingArguments(
    output_dir="./gpt2-tinystories",
    overwrite_output_dir=True,

    num_train_epochs=3,                # ‚¨ÖÔ∏è higher epochs
    per_device_train_batch_size=4,     # ‚¨ÖÔ∏è speed boost
    gradient_accumulation_steps=4,     # effective batch = 16

    learning_rate=5e-5,
    fp16=True,

    save_strategy="no",                # ‚¨ÖÔ∏è HUGE time saver
    logging_steps=1000,

    report_to="none"
)

from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # VERY IMPORTANT for GPT-2
)

from transformers import Trainer

train_subset = tokenized_dataset["train"] \
    .shuffle(seed=42) \
    .select(range(30000))

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_subset,
    data_collator=data_collator
)
# train_dataset=tokenized_dataset["train"]

trainer.train()

import re

def trim_to_last_sentence(text):
    matches = list(re.finditer(r'[.!?]', text))
    if matches:
        last_end = matches[-1].end()
        return text[:last_end]
    return text

#prompt = "Once upon a time"
#inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

#outputs = model.generate(
 #   **inputs,
  #  max_length=1000,
   # temperature=0.8,
    #top_p=0.9,
    #repetition_penalty=1.2,
    #no_repeat_ngram_size=3,
    #eos_token_id=tokenizer.eos_token_id,
    #do_sample=True
#)

#raw_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
#final_text = trim_to_last_sentence(raw_text)

#print(final_text)

#for i in range(3):
 #   print(f"\n--- STORY {i+1} ---")
  #  outputs = model.generate(
   #     **inputs,
    #    max_length=1000,
     #   temperature=0.75,
      #  top_p=0.9,
       # repetition_penalty=1.2,
       # no_repeat_ngram_size=3,
        #eos_token_id=tokenizer.eos_token_id,
        #do_sample=True
    #)
    #text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    #print(trim_to_last_sentence(text))

base_model = GPT2LMHeadModel.from_pretrained("gpt2").to("cuda")
base_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

inputs = base_tokenizer(prompt, return_tensors="pt").to("cuda")

outputs = base_model.generate(
    **inputs,
    max_new_tokens=500,           # not 1000
    temperature=0.7,              # lower randomness
    top_p=0.9,
    repetition_penalty=1.3,       # üîë prevents looping
    no_repeat_ngram_size=3,       # üîë blocks phrase repetition
    eos_token_id=base_tokenizer.eos_token_id,
    pad_token_id=base_tokenizer.eos_token_id,
    do_sample=True
)


print(base_tokenizer.decode(outputs[0], skip_special_tokens=True))

#with open("generated_samples.txt", "w", encoding="utf-8") as f:
 #   for i in range(5):

        # üîë Ensure inputs are on the SAME device as the model
  #      inputs = {k: v.to(model.device) for k, v in inputs.items()}

   #     outputs = model.generate(
    #        **inputs,
     #       max_new_tokens=1000,          # use this instead of max_length
      #      temperature=0.75,
       #     top_p=0.9,
        #    repetition_penalty=1.2,
         #   no_repeat_ngram_size=3,
          #  eos_token_id=tokenizer.eos_token_id,
           # pad_token_id=tokenizer.eos_token_id,
            #do_sample=True
        #)

        #raw_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        #final_text = trim_to_last_sentence(raw_text)

        # ‚úÖ PRINT output
        #print(f"\n--- GENERATED STORY {i+1} ---\n")
        #print(final_text)

        # ‚úÖ SAVE output
        #f.write(final_text + "\n\n")

demo_prompts = [
    "Once upon a time, there was a kind little girl who lived in a small village.",
    "Once upon a time, a small animal learned an important lesson about friendship.",
    "Once upon a time, a child discovered something magical in the forest."
]

generation_kwargs = dict(
    max_new_tokens=200,            # enough for full story
    temperature=0.55,              # reduces drift
    top_p=0.9,
    repetition_penalty=1.2,
    no_repeat_ngram_size=3,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.eos_token_id,
    do_sample=True
)

with open("generated_samples.txt", "w", encoding="utf-8") as f:
    for i, prompt in enumerate(demo_prompts, 1):

        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        outputs = model.generate(
            **inputs,
            **generation_kwargs
        )

        raw_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        final_text = trim_to_last_sentence(raw_text)

        print(f"\n--- DEMO STORY {i} ---\n")
        print(final_text)

        f.write(f"--- DEMO STORY {i} ---\n")
        f.write(final_text + "\n\n")